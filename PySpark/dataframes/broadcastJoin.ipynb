{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd3157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"ShobhitApp\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad26e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = spark.read.json(\"/public/addresses/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc758e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "l = list (range(10000000))\n",
    "new_list = random.sample(l,k=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c81545",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76cc693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+------+------+--------------+---------+--------------------+\n",
      "|             address|               email|first_name|gender|    id|    ip_address|last_name|       phone_numbers|\n",
      "+--------------------+--------------------+----------+------+------+--------------+---------+--------------------+\n",
      "|[Honolulu, 96840,...|lbreyt0@tripadvis...|  L;urette|Female|900001| 80.24.165.223|    Breyt|[213-896-1319, 21...|\n",
      "|[Charlotte, 28278...|    nkilrow1@last.fm|     Nixie|Female|900002|169.186.205.65|   Kilrow|[801-204-0578, 60...|\n",
      "+--------------------+--------------------+----------+------+------+--------------+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_big.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4533a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|     id|\n",
      "+-------+\n",
      "|2464728|\n",
      "|9165286|\n",
      "|3931668|\n",
      "|8916218|\n",
      "|3719980|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = [\"id\"]\n",
    "df_small = rdd.map(lambda x: (x, )).toDF(col)\n",
    "df_small.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9deefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ecc3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [id#10L], [id#55L], Inner\n",
      ":- *(2) Sort [id#10L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(id#10L, 200)\n",
      ":     +- *(1) Project [address#6, email#7, first_name#8, gender#9, id#10L, ip_address#11, last_name#12, phone_numbers#13]\n",
      ":        +- *(1) Filter isnotnull(id#10L)\n",
      ":           +- *(1) FileScan json [address#6,email#7,first_name#8,gender#9,id#10L,ip_address#11,last_name#12,phone_numbers#13] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/addresses], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<address:struct<city:string,postal_code:string,state:string,street:string>,email:string,fir...\n",
      "+- *(4) Sort [id#55L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(id#55L, 200)\n",
      "      +- *(3) Filter isnotnull(id#55L)\n",
      "         +- Scan ExistingRDD[id#55L]\n"
     ]
    }
   ],
   "source": [
    "df_big.join(df_small,df_big.id ==  df_small.id,\"inner\") \\\n",
    "     .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237edb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35b3d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big.coalesce(1).write.save(path='/user/itv000197/new_big_output', format='parquet', mode='append', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "480bd198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.join.preferSortMergeJoin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f44d0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3fad4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'false'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.join.preferSortMergeJoin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fb7097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "ShuffledHashJoin [id#10L], [id#55L], Inner, BuildLeft\n",
      ":- Exchange hashpartitioning(id#10L, 200)\n",
      ":  +- *(1) Project [address#6, email#7, first_name#8, gender#9, id#10L, ip_address#11, last_name#12, phone_numbers#13]\n",
      ":     +- *(1) Filter isnotnull(id#10L)\n",
      ":        +- *(1) FileScan json [address#6,email#7,first_name#8,gender#9,id#10L,ip_address#11,last_name#12,phone_numbers#13] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/addresses], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<address:struct<city:string,postal_code:string,state:string,street:string>,email:string,fir...\n",
      "+- Exchange hashpartitioning(id#55L, 200)\n",
      "   +- *(2) Filter isnotnull(id#55L)\n",
      "      +- Scan ExistingRDD[id#55L]\n"
     ]
    }
   ],
   "source": [
    "df_big.join(df_small,df_big.id ==  df_small.id,\"inner\") \\\n",
    "     .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f94e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [id#10L], [id#55L], Inner, BuildRight\n",
      ":- *(2) Project [address#6, email#7, first_name#8, gender#9, id#10L, ip_address#11, last_name#12, phone_numbers#13]\n",
      ":  +- *(2) Filter isnotnull(id#10L)\n",
      ":     +- *(2) FileScan json [address#6,email#7,first_name#8,gender#9,id#10L,ip_address#11,last_name#12,phone_numbers#13] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/public/addresses], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<address:struct<city:string,postal_code:string,state:string,street:string>,email:string,fir...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]))\n",
      "   +- *(1) Filter isnotnull(id#55L)\n",
      "      +- Scan ExistingRDD[id#55L]\n"
     ]
    }
   ],
   "source": [
    "df_big.join(broadcast(df_small),df_big.id ==  df_small.id,\"inner\") \\\n",
    "     .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22468c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
