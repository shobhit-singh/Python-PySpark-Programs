{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b11fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"ShobhitApp\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2295b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[\"1\"]]\n",
    "df=spark.createDataFrame(data,[\"id\"])\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "#current_date() & current_timestamp()\n",
    "df = df.withColumn(\"current_date\",current_date()) \\\n",
    "  .withColumn(\"current_timestamp\",current_timestamp()) \\\n",
    ".withColumn(\"last_day_prev_month\", last_day(add_months(col(\"current_date\"), -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42f61acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+-------------------+\n",
      "| id|current_date|   current_timestamp|last_day_prev_month|\n",
      "+---+------------+--------------------+-------------------+\n",
      "|  1|  2022-04-21|2022-04-21 01:47:...|         2022-03-31|\n",
      "+---+------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d996eb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|   id|id_new|\n",
      "+-----+------+\n",
      "|00001|     1|\n",
      "| 0011|    11|\n",
      "|  002|     2|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"00001\"],[\"0011\"],[\"002\"]]\n",
    "df=spark.createDataFrame(data,[\"id\"])\n",
    "df.withColumn('id_new', regexp_replace('id', r'^[0]*', '')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ece02",
   "metadata": {},
   "source": [
    "##### Extended Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b98357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec27590",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f437f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- Join Inner, (cast(emp_dept_id#43 as bigint) = dept_id#76L)\n",
      "   :- LogicalRDD [emp_id#39L, name#40, superior_emp_id#41L, year_joined#42, emp_dept_id#43, gender#44, salary#45L], false\n",
      "   +- LogicalRDD [dept_name#75, dept_id#76L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "emp_id: bigint, name: string, superior_emp_id: bigint, year_joined: string, emp_dept_id: string, gender: string, salary: bigint, dept_name: string, dept_id: bigint\n",
      "Project [emp_id#39L, name#40, superior_emp_id#41L, year_joined#42, emp_dept_id#43, gender#44, salary#45L, dept_name#75, dept_id#76L]\n",
      "+- Join Inner, (cast(emp_dept_id#43 as bigint) = dept_id#76L)\n",
      "   :- LogicalRDD [emp_id#39L, name#40, superior_emp_id#41L, year_joined#42, emp_dept_id#43, gender#44, salary#45L], false\n",
      "   +- LogicalRDD [dept_name#75, dept_id#76L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (cast(emp_dept_id#43 as bigint) = dept_id#76L)\n",
      ":- Filter isnotnull(emp_dept_id#43)\n",
      ":  +- LogicalRDD [emp_id#39L, name#40, superior_emp_id#41L, year_joined#42, emp_dept_id#43, gender#44, salary#45L], false\n",
      "+- Filter isnotnull(dept_id#76L)\n",
      "   +- LogicalRDD [dept_name#75, dept_id#76L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [cast(emp_dept_id#43 as bigint)], [dept_id#76L], Inner\n",
      ":- *(2) Sort [cast(emp_dept_id#43 as bigint) ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(cast(emp_dept_id#43 as bigint), 200)\n",
      ":     +- *(1) Filter isnotnull(emp_dept_id#43)\n",
      ":        +- Scan ExistingRDD[emp_id#39L,name#40,superior_emp_id#41L,year_joined#42,emp_dept_id#43,gender#44,salary#45L]\n",
      "+- *(4) Sort [dept_id#76L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(dept_id#76L, 200)\n",
      "      +- *(3) Filter isnotnull(dept_id#76L)\n",
      "         +- Scan ExistingRDD[dept_name#75,dept_id#76L]\n"
     ]
    }
   ],
   "source": [
    "res.select('*').explain(extended=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
